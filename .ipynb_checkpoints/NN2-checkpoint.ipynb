{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc86701d-f272-4f2f-b8e1-15479272d04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections.abc import Iterable\n",
    "import numbers\n",
    "import jax.numpy as jnp\n",
    "from jax import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c09587f8-252b-4051-b2b7-207a9a3a9cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_with_backpropagation():\n",
    "    def identity(x):\n",
    "        return x\n",
    "        \n",
    "    def __init__(self, shape, activations = None): #len(activations)+1=len(shape)\n",
    "        fed_values = []\n",
    "        activation_values = []\n",
    "        for layer_size in shape:\n",
    "            fed_values.append(np.array([0]*layer_size))\n",
    "            activation_values.append(np.array([0]*layer_size))\n",
    "        self.fed_values = fed_values\n",
    "        self.activation_values = activation_values\n",
    "        #first layer values are set so that indexes match\n",
    "        weights = [0]\n",
    "        biases = [0]\n",
    "        for i in range(1, len(self.fed_values)):\n",
    "            n = len(self.fed_values[i])\n",
    "            m = len(self.fed_values[i-1])\n",
    "            #initialising with random values\n",
    "            weight_matrix = np.random.normal(0,1,(n,m))\n",
    "            weights.append(weight_matrix)\n",
    "            bias_vector = np.random.normal(0,1,n)\n",
    "            biases.append(bias_vector)\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "        \n",
    "        if activations:\n",
    "            self.activations = [0] + [np.vectorize(activation) for activation in activations]\n",
    "            self.activations_sv = [0] + [activation for activation in activations]\n",
    "        else:\n",
    "            self.activations = [0] + [np.vectorize(MLP_with_backpropagation.identity)] * (len(shape) - 1)\n",
    "            self.activations_sv = [0] + [MLP_with_backpropagation.identity] * (len(shape) - 1)\n",
    "            \n",
    "    #2 functions below are only for technical purposes\n",
    "    def is_iterable(obj):\n",
    "        return isinstance(obj, Iterable)\n",
    "\n",
    "    def is_numeric_vector_of_given_length(supposed_vector, length):\n",
    "         if not MLP_with_backpropagation.is_iterable(supposed_vector):\n",
    "             return False\n",
    "         if len(supposed_vector) != length:\n",
    "             return False\n",
    "         for el in supposed_vector:\n",
    "             if not isinstance(el, numbers.Number):\n",
    "                 return False\n",
    "         return True\n",
    "        \n",
    "    def set_input(self, inputt):\n",
    "        if not MLP_with_backpropagation.is_numeric_vector_of_given_length(inputt, len(self.fed_values[0])):\n",
    "            print(\"Wrong input size or type, it is supposed to be a numerical list or a 1D np.array of length of 1st layer\")\n",
    "            return False\n",
    "        self.fed_values[0] = np.array(inputt)\n",
    "        self.activation_values[0] = np.array(inputt) #input is input\n",
    "        return True\n",
    "        \n",
    "    def feed_forward(self):\n",
    "        for i in range(1, len(self.fed_values)):\n",
    "            self.fed_values[i] = np.dot(self.weights[i], self.activation_values[i-1]) + self.biases[i] ##can do if statement if activation is function of layer not neuron\n",
    "            self.activation_values[i] = self.activations[i](self.fed_values[i])\n",
    "\n",
    "    def predict(self, x):\n",
    "        if not self.set_input(x):\n",
    "            return False\n",
    "        self.feed_forward()\n",
    "        return self.activation_values[-1]\n",
    "        \n",
    "    #manual setting of weights and biases    \n",
    "    def set_weights(self, weights):\n",
    "        self.weights = weights\n",
    "\n",
    "    def set_biases(self, biases):\n",
    "        self.biases = biases\n",
    "\n",
    "    def squared_error(pred, expected):\n",
    "        return (pred - expected)**2\n",
    "\n",
    "    \n",
    "    #returns a pair first element is for weights second for biases\n",
    "    def derivative(self, inputt, expected):\n",
    "        self.predict(inputt)\n",
    "        dx = 10**(-6)\n",
    "        weight_grad = [0] + [np.zeros(self.weights[i].shape) for i in range(1, len(self.weights))]\n",
    "        bias_grad = [0] + [np.zeros(len(self.biases[i])) for i in range(1, len(self.biases))]\n",
    "        neuron_activation_grad = [0] + [np.zeros(len(self.activation_values[i])) for i in range(1, len(self.activation_values))]\n",
    "        neuron_fed_grad = [0] + [np.zeros(len(self.fed_values[i])) for i in range(1, len(self.fed_values))]\n",
    "        last_layer = True\n",
    "        for i in range(len(self.fed_values)-1, 0, -1):\n",
    "            #derivatives in respect to neuron activation values\n",
    "            for j in range(len(self.activation_values[i])):\n",
    "                if last_layer:\n",
    "                    x_0 = self.activation_values[i][j]\n",
    "                    y_0 = expected[j]\n",
    "                    dc_da = grad(MLP_with_backpropagation.squared_error, argnums = 0)\n",
    "                    neuron_activation_grad[i][j] = dc_da(x_0, y_0)\n",
    "                else:\n",
    "                    neuron_activation_grad[i][j] = sum([neuron_fed_grad[i+1][k] * self.weights[i+1][k][j] for k in range(len(neuron_fed_grad[i+1]))])\n",
    "                \n",
    "                da_dz = grad(self.activations_sv[i], argnums = 0)                                       \n",
    "                neuron_fed_grad[i][j] = neuron_activation_grad[i][j]*da_dz(self.fed_values[i][j])\n",
    "                bias_grad[i][j] = neuron_fed_grad[i][j]\n",
    "                \n",
    "                for k in range(len(self.weights[i][j])):\n",
    "                    weight_grad[i][j][k] = neuron_fed_grad[i][j] * self.activation_values[i-1][k]\n",
    "            last_layer = False\n",
    "        return (weight_grad, bias_grad)\n",
    "                \n",
    "        \n",
    "\n",
    "    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "115e183f-0cbc-4ae7-b929-0b55cd063814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i in range(10,0, -1):\n",
    "    print(i)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "359ebc93-994d-47c8-93fe-95ef9b69ce1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = MLP_with_backpropagation([3,4,5])\n",
    "w_grad, b_grad = network.derivative(np.array([1,2,3]), np.array([3,4,5,6,7]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "52348486-8b7a-4963-86f0-fa7a8d2151e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, array([ 29.57564735,  43.02214813, -20.65423393,  17.97513199]), array([-15.18190479,   0.3594265 , -27.13799858, -12.19403458,\n",
      "         0.27427387])]\n"
     ]
    }
   ],
   "source": [
    "print(b_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "febe7331-927c-4232-b8b2-7dbaccad9ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_squared(x, y):\n",
    "    return (x-y)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a1ff9fcc-16b7-4e9f-9f4a-5e33d90fc546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(difference_squared(7,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "758553f6-e724-495f-92fb-811ef3b7fdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56.0\n"
     ]
    }
   ],
   "source": [
    "df_dx = grad(difference_squared, argnums = 0)\n",
    "val = df_dx(7.0,3.0)\n",
    "print(df_dx(7.0,3.0)*7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5193953f-4119-4333-9ecc-51ba569a8714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
