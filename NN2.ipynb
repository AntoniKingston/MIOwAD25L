{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "cc86701d-f272-4f2f-b8e1-15479272d04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections.abc import Iterable\n",
    "import numbers\n",
    "import jax.numpy as jnp\n",
    "from jax import grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c09587f8-252b-4051-b2b7-207a9a3a9cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_with_backpropagation():\n",
    "    def identity(x):\n",
    "        return x\n",
    "        \n",
    "    def __init__(self, shape, activations = None): #len(activations)+1=len(shape)\n",
    "        fed_values = []\n",
    "        activation_values = []\n",
    "        for layer_size in shape:\n",
    "            fed_values.append(np.array([0]*layer_size))\n",
    "            activation_values.append(np.array([0]*layer_size))\n",
    "        self.fed_values = fed_values\n",
    "        self.activation_values = activation_values\n",
    "        #first layer values are set so that indexes match\n",
    "        weights = [0]\n",
    "        biases = [0]\n",
    "        for i in range(1, len(self.fed_values)):\n",
    "            n = len(self.fed_values[i])\n",
    "            m = len(self.fed_values[i-1])\n",
    "            #initialising with random values\n",
    "            weight_matrix = np.random.normal(0,1,(n,m))\n",
    "            weights.append(weight_matrix)\n",
    "            bias_vector = np.random.normal(0,1,n)\n",
    "            biases.append(bias_vector)\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "        \n",
    "        if activations:\n",
    "            self.activations = [0] + [np.vectorize(activation) for activation in activations]\n",
    "            self.activations_sv = [0] + [activation for activation in activations]\n",
    "        else:\n",
    "            self.activations = [0] + [np.vectorize(MLP_with_backpropagation.identity)] * (len(shape) - 1)\n",
    "            self.activations_sv = [0] + [MLP_with_backpropagation.identity] * (len(shape) - 1)\n",
    "            \n",
    "    #2 functions below are only for technical purposes\n",
    "    def is_iterable(obj):\n",
    "        return isinstance(obj, Iterable)\n",
    "\n",
    "    def is_numeric_vector_of_given_length(supposed_vector, length):\n",
    "         if not MLP_with_backpropagation.is_iterable(supposed_vector):\n",
    "             return False\n",
    "         if len(supposed_vector) != length:\n",
    "             return False\n",
    "         for el in supposed_vector:\n",
    "             if not isinstance(el, numbers.Number):\n",
    "                 return False\n",
    "         return True\n",
    "        \n",
    "    def set_input(self, inputt):\n",
    "        if not MLP_with_backpropagation.is_numeric_vector_of_given_length(inputt, len(self.fed_values[0])):\n",
    "            print(\"Wrong input size or type, it is supposed to be a numerical list or a 1D np.array of length of 1st layer\")\n",
    "            return False\n",
    "        self.fed_values[0] = np.array(inputt)\n",
    "        self.activation_values[0] = np.array(inputt) #input is input\n",
    "        return True\n",
    "        \n",
    "    def feed_forward(self):\n",
    "        for i in range(1, len(self.fed_values)):\n",
    "            self.fed_values[i] = np.dot(self.weights[i], self.activation_values[i-1]) + self.biases[i] ##can do if statement if activation is function of layer not neuron\n",
    "            self.activation_values[i] = self.activations[i](self.fed_values[i])\n",
    "\n",
    "    def predict(self, x):\n",
    "        if not self.set_input(x):\n",
    "            return False\n",
    "        self.feed_forward()\n",
    "        return self.activation_values[-1]\n",
    "\n",
    "    def predict_multiple(self, x):\n",
    "        y = []\n",
    "        for el in x:\n",
    "            y.append(self.predict(el))\n",
    "        return np.array(y)\n",
    "        \n",
    "    #manual setting of weights and biases    \n",
    "    def set_weights(self, weights):\n",
    "        self.weights = weights\n",
    "\n",
    "    def set_biases(self, biases):\n",
    "        self.biases = biases\n",
    "\n",
    "    def squared_error(pred, expected):\n",
    "        return (pred - expected)**2\n",
    "\n",
    "    \n",
    "    #returns a pair first element is for weights second for biases\n",
    "    def derivative(self, inputt, expected):\n",
    "        self.predict(inputt)\n",
    "        dx = 10**(-6)\n",
    "        weight_grad = [0] + [np.zeros(self.weights[i].shape) for i in range(1, len(self.weights))]\n",
    "        bias_grad = [0] + [np.zeros(len(self.biases[i])) for i in range(1, len(self.biases))]\n",
    "        neuron_activation_grad = [0] + [np.zeros(len(self.activation_values[i])) for i in range(1, len(self.activation_values))]\n",
    "        neuron_fed_grad = [0] + [np.zeros(len(self.fed_values[i])) for i in range(1, len(self.fed_values))]\n",
    "        last_layer = True\n",
    "        for i in range(len(self.fed_values)-1, 0, -1):\n",
    "            #derivatives in respect to neuron activation values\n",
    "            for j in range(len(self.activation_values[i])):\n",
    "                if last_layer:\n",
    "                    x_0 = self.activation_values[i][j]\n",
    "                    y_0 = expected[j]\n",
    "                    dc_da = grad(MLP_with_backpropagation.squared_error, argnums = 0)\n",
    "                    neuron_activation_grad[i][j] = dc_da(x_0, y_0)\n",
    "                else:\n",
    "                    neuron_activation_grad[i][j] = sum([neuron_fed_grad[i+1][k] * self.weights[i+1][k][j] for k in range(len(neuron_fed_grad[i+1]))])\n",
    "                \n",
    "                da_dz = grad(self.activations_sv[i], argnums = 0)                                       \n",
    "                neuron_fed_grad[i][j] = neuron_activation_grad[i][j]*da_dz(self.fed_values[i][j])\n",
    "                bias_grad[i][j] = neuron_fed_grad[i][j]\n",
    "                \n",
    "                for k in range(len(self.weights[i][j])):\n",
    "                    weight_grad[i][j][k] = neuron_fed_grad[i][j] * self.activation_values[i-1][k]\n",
    "            last_layer = False\n",
    "        return (weight_grad, bias_grad)\n",
    "\n",
    "    def add_2_lists_of_np_arrays(l1, l2):\n",
    "        for i in range(len(l1)):\n",
    "            l1[i] += l2[i]\n",
    "        return l1\n",
    "        \n",
    "    def subtract_2_lists_of_np_arrays(l1, l2):\n",
    "        for i in range(len(l1)):\n",
    "            l1[i] -= l2[i]\n",
    "        return l1\n",
    "        \n",
    "    def create_0filled_list_of_np_arrays(lista):\n",
    "        listr = [0]\n",
    "        for i in range(1, len(lista)):\n",
    "            listr.append(np.zeros(lista[i].shape))\n",
    "        return listr\n",
    "\n",
    "    def multiply_list_elementwise(lista, factor):\n",
    "        for i in range(len(lista)):\n",
    "            lista[i] = lista[i] * factor\n",
    "        return lista\n",
    "\n",
    "    def learn(self, x, y, epochs, lr=0.0001, batch_size = max(1,int(len(x)/100))):\n",
    "            for i in range(epochs):\n",
    "                idx = 0\n",
    "                while(idx + batch_size < len(x)):\n",
    "                    batchx = x[idx:idx+batch_size]\n",
    "                    batchy = y[idx:idx+batch_size]\n",
    "                    for i in range(batch_size):\n",
    "                        local_weight_gradient, local_bias_gradient = self.derivative(batchx[i], batchy[i])\n",
    "                        if i==0:\n",
    "                            avg_weight_gradient = MLP_with_backpropagation.create_0filled_list_of_np_arrays(local_weight_gradient)\n",
    "                            avg_bias_gradient = MLP_with_backpropagation.create_0filled_list_of_np_arrays(local_bias_gradient)\n",
    "                        avg_weight_gradient = MLP_with_backpropagation.add_2_lists_of_np_arrays(avg_weight_gradient, local_weight_gradient)\n",
    "                        avg_bias_gradient = MLP_with_backpropagation.add_2_lists_of_np_arrays(avg_bias_gradient, local_bias_gradient)\n",
    "                    avg_weight_gradient = MLP_with_backpropagation.multiply_list_elementwise(avg_weight_gradient, 1/batch_size)\n",
    "                    avg_bias_gradient = MLP_with_backpropagation.multiply_list_elementwise(avg_bias_gradient, 1/batch_size)\n",
    "                    self.weights = MLP_with_backpropagation.subtract_2_lists_of_np_arrays(self.weights, MLP_with_backpropagation.multiply_list_elementwise(avg_weight_gradient, lr))\n",
    "                    self.biases = MLP_with_backpropagation.subtract_2_lists_of_np_arrays(self.biases, MLP_with_backpropagation.multiply_list_elementwise(avg_bias_gradient, lr))\n",
    "                    idx += batch_size\n",
    "        \n",
    "\n",
    "    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e183f-0cbc-4ae7-b929-0b55cd063814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE0_learn_MSE1(network, X_train, Y_train, X_test, Y_test):\n",
    "    max_x = max(X_train)\n",
    "    min_x = min(X_train)\n",
    "    max_y = max(Y_train)\n",
    "    min_y = min(Y_train)\n",
    "    x_scl = max_x - min_x\n",
    "    y_scl = max_y - min_y\n",
    "    X_tr_scl = X_train / x_scl\n",
    "    Y_tr_scl = Y_train / y_scl\n",
    "    X_ts_scl = X_test / x_scl\n",
    "    Y_ts_scl = Y_test / y_scl\n",
    "    pred0 = network.predict_multiple(X_ts_scl.reshape(-1,1)) * y_scl\n",
    "    MSE0 = sum((Y_test - pred0.reshape(-1))**2)\n",
    "    network.learn(X_tr_scl.reshape(-1,1), Y_tr_scl.reshape(-1,1), 50)\n",
    "    pred1 = network.predict_multiple(X_ts_scl.reshape(-1,1))* y_scl\n",
    "    MSE1 = sum((Y_test - pred1.reshape(-1))**2)\n",
    "    return MSE0, MSE1\n",
    "    \n",
    "network = MLP_with_backpropagation([1,5,1])\n",
    "train = pd.read_csv(\"data/regression/square-simple-training.csv\")\n",
    "test = pd.read_csv(\"data/regression/square-simple-test.csv\")\n",
    "\n",
    "X_train = train['x'].values\n",
    "Y_train = train['y'].values\n",
    "X_test = test['x'].values\n",
    "Y_test = test['y'].values\n",
    "\n",
    "MSE0, MSE1 = MSE0_learn_MSE1(network, X_train, Y_train, X_test, Y_test)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "359ebc93-994d-47c8-93fe-95ef9b69ce1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(27672589.18610299)"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "52348486-8b7a-4963-86f0-fa7a8d2151e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(911699.2977257003)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "febe7331-927c-4232-b8b2-7dbaccad9ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
      " nan nan nan nan nan nan nan nan nan nan]\n"
     ]
    }
   ],
   "source": [
    "print(MSE1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a1ff9fcc-16b7-4e9f-9f4a-5e33d90fc546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.47281368  2.6941238  -0.75494902 -1.11110316 -0.09680387] + [ 0.81893917 -0.58106128 -0.24249267 -0.56237396 -1.28480126]\n",
      "[-1.50452024] + [-0.36423103]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0,\n",
       "  array([[-0.49614689],\n",
       "         [ 4.84981966],\n",
       "         [ 5.09489298],\n",
       "         [-8.459342  ],\n",
       "         [ 1.36545646]]),\n",
       "  array([[ -1.9858959 , -12.12370167,   5.72282427,   9.60157933,\n",
       "            7.92696302]])],\n",
       " [0,\n",
       "  array([-0.24807344,  2.42490983,  2.54744649, -4.229671  ,  0.68272823]),\n",
       "  array([-5.73750257])])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "network = MLP_with_backpropagation([1,5,1])\n",
    "network.derivative([2], [1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "758553f6-e724-495f-92fb-811ef3b7fdfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.47281368  2.6941238  -0.75494902 -1.11110316 -0.09680387] + [ 0.81893917 -0.58106128 -0.24249267 -0.56237396 -1.28480126]\n",
      "[-1.50452024] + [-0.36423103]\n",
      "([0, array([[-0.49614689],\n",
      "       [ 4.84981966],\n",
      "       [ 5.09489298],\n",
      "       [-8.459342  ],\n",
      "       [ 1.36545646]]), array([[ -1.9858959 , -12.12370167,   5.72282427,   9.60157933,\n",
      "          7.92696302]])], [0, array([-0.24807344,  2.42490983,  2.54744649, -4.229671  ,  0.68272823]), array([-5.73750257])])\n"
     ]
    }
   ],
   "source": [
    "print(network.derivative([2], [1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce74986-fe41-475b-a1de-95bfe59a62ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
