{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cc86701d-f272-4f2f-b8e1-15479272d04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections.abc import Iterable\n",
    "import numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c09587f8-252b-4051-b2b7-207a9a3a9cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_with_backpropagation():\n",
    "    def identity(x):\n",
    "        return x\n",
    "        \n",
    "    def __init__(self, shape, activations = None): #len(activations)+1=len(shape)\n",
    "        fed_values = []\n",
    "        activation_values = []\n",
    "        for layer_size in shape:\n",
    "            fed_values.append(np.array([0]*layer_size))\n",
    "            activation_values.append(np.array([0]*layer_size))\n",
    "        self.fed_values = fed_values\n",
    "        self.activation_values = activation_values\n",
    "        weights = []\n",
    "        biases = []\n",
    "        for i in range(1, len(self.fed_values)):\n",
    "            n = len(self.fed_values[i])\n",
    "            m = len(self.fed_values[i-1])\n",
    "            #initialising with random values\n",
    "            weight_matrix = np.random.normal(0,1,(n,m))\n",
    "            weights.append(weight_matrix)\n",
    "            bias_vector = np.random.normal(0,1,n)\n",
    "            biases.append(bias_vector)\n",
    "        self.weights = weights\n",
    "        self.biases = biases\n",
    "        \n",
    "        if activations:\n",
    "            self.activations = [np.vectorize(activation) for activation in activations]\n",
    "        else:\n",
    "            self.activations = [np.vectorize(MLP_with_backpropagation.identity)] * (len(shape) - 1)\n",
    "            \n",
    "    #2 functions below are only for technical purposes\n",
    "    def is_iterable(obj):\n",
    "        return isinstance(obj, Iterable)\n",
    "\n",
    "    def is_numeric_vector_of_given_length(supposed_vector, length):\n",
    "         if not MLP_with_backpropagation.is_iterable(supposed_vector):\n",
    "             return False\n",
    "         if len(supposed_vector) != length:\n",
    "             return False\n",
    "         for el in supposed_vector:\n",
    "             if not isinstance(el, numbers.Number):\n",
    "                 return False\n",
    "         return True\n",
    "        \n",
    "    def set_input(self, inputt):\n",
    "        if not MLP_with_backpropagation.is_numeric_vector_of_given_length(inputt, len(self.fed_values[0])):\n",
    "            print(\"Wrong input size or type, it is supposed to be a numerical list or a 1D np.array of length of 1st layer\")\n",
    "            return False\n",
    "        self.fed_values[0] = np.array(inputt)\n",
    "        self.activation_values[0] = np.array(inputt) #input is input\n",
    "        return True\n",
    "        \n",
    "    def feed_forward(self):\n",
    "        for i in range(1, len(self.fed_values)):\n",
    "            self.fed_values[i] = np.dot(self.weights[i-1], self.activation_values[i-1]) + self.biases[i-1] ##can do if statement if activation is function of layer not neuron\n",
    "            self.activation_values[i] = self.activations[i-1](self.fed_values[i])\n",
    "\n",
    "    def predict(self, x):\n",
    "        if not self.set_input(x):\n",
    "            return False\n",
    "        self.feed_forward()\n",
    "        return self.activation_values[-1]\n",
    "        \n",
    "    #manual setting of weights and biases    \n",
    "    def set_weights(self, weights):\n",
    "        self.weights = weights\n",
    "\n",
    "    def set_biases(self, biases):\n",
    "        self.biases = biases\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "115e183f-0cbc-4ae7-b929-0b55cd063814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18]\n"
     ]
    }
   ],
   "source": [
    "network = MLP_with_backpropagation([1,1,1])\n",
    "network.set_weights([np.array([[2]]), np.array([[2]])])\n",
    "network.set_biases(np.array([4,2]))\n",
    "print(network.predict([2]))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "359ebc93-994d-47c8-93fe-95ef9b69ce1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([1,2,3])\n",
    "y = np.array([4,5,6])\n",
    "x+y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52348486-8b7a-4963-86f0-fa7a8d2151e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
